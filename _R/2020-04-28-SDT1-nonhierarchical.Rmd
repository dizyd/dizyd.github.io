---
layout: single
title: "Signal Detection Model with STAN - Part 1"
excerpt: "Modeling recognition data with a signal detection model using Stan"
date: 2020-04-28
tags:
  - cognitive modeling
  - Stan
  - R
  
# status: process
# published: true
# status: publish
# published: true
permalink: /posts/2020/04/SDT-1/
classes: wide
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo    = TRUE,
                      cache   = FALSE,
                      warning = FALSE,
                      message = FALSE,
                      fig.align = 'center',
                      fig.width = 10,
                      fig.height = 4,
                      dpi = 600)

```

This blog post is supposed to be the first in a series of blog post comparing the *signal detection theory * model (SDT) with a *two-high-threshold* model (2HTM) of recognition.


I wrote this blog post for three reasons. First, I wanted to learn more about the modeling of recognition and memory data. Second, I wanted to learn more about Stan, since I mostly use JAGS in my own research. Finally, I also wanted to practice writing, since I take forever when writing my own articles. So the reasons for this blog are rather selfish. However, if anyone ever finds this blog post and finds it helpful, that would be even better!

## About this blog post

In this blog post you are going to read, I will use a *non-hierarchical SDT* model to investigate the data from a recogntion experiment. In following blog posts, I will extend this model to account for differences between individuals as well as differences between stimulus sets. Then I will model the same data with a 2HTM and finally compare both models with each other. 

## Setup 

At the beginning, I load the packages I need for this analysis. However, before we start with the actual analysis and modeling, I first want to give a short introduction into signal detection theory.

```{r packages, echo=TRUE, message=FALSE, warning=FALSE}
library(tidyverse) # contains ggplot, dplyr etc
library(cmdstanr)
library(bayesplot)
library(patchwork)
library(kableExtra)
library(knitr)
library(latex2exp)
library(tidybayes)
library(posterior)

# Set bayesplot theme
bayesplot_theme_set(theme_bw())

theme_set(theme_bw() + theme(plot.title = element_text(hjust = 0.5)))
```


## Signal detection model 

Signal detection theory (SDT) may be applied to any area of psychology in which two different types of stimuli must be discriminated. It was first applied in studies of perception, where subjects had to discriminated between signals (stimuli) and noise (no stimuli). However, SDT is also often used to describe memory recognition task, where subjects have to discriminate between old (signal) and new items (noise). 

The idea behind SDT is that signal and noise trials can be represented as values along a uni-dimensional continuous strength dimension, where signal trials are assumed to have a greater strength than noise trials. According to SDT, people produce "old" or "new" decisions by comparing the strength of the current trial to a fixed threshold. If the strength exceeds this threshold, the response is "old", otherwise the response is "new". The strength of signal and noise trials is assumed to be normally distributed with different means (but the mean of the noise distributions is assumed to be equal to 0), but the same variance (which is fixed to 1), which can be expressed as:

$$
\begin{aligned}
noise &\sim N(0,1)  \\[.5em]
signal &\sim N(d,1)
\end{aligned}
$$

where $d$ is the **discriminability** or **sensitivity** parameter, which goes from $-\infty$ to $+ \infty $. This parameter $d$ corresponds to the diStance between means of the noise and the signal distribution in Standard deviation units. A value of 0 indicates an inability to distinguish signals from noise, whereas larger values indicate a correspondingly greater ability to distinguish signals from noise. Negative values are also possible, but are harder to interpret. They are most often thought of as a sampling error or response confusion (i.e., responding "old" when intending to respond "new", and vice versa, for inStance by confusing the corresponding buttons). 

Another parameter is $c$, the **bias** parameter. Positive values of $c$ correspond to a bias towards saying *new* and negative values correspond to a bias towards saying *old*. 

These two parameters can then be directly translated into hit (HR) and false-alarm rates (FR) via:

$$
\begin{aligned}
HR &= \phi(0.5 \times d - c) \\[.5em]
FR &= \phi(- 0.5 \times d - c) 
\end{aligned}
$$

where, $\phi$ is the cumulative density function of the Standard normal distribution. HR and FR  map naturally to the data pattern we observe in a recognition memory experiment, were participants see either an old (signal trial) or a new item (noise trial), and then have to respond by pressing the corresponding button:

```{r echo=FALSE}
data.frame("R"   = c("Old","New"),
           "ST"  = c("Old","Old"),
           "NT"  = c("New", "New")) %>% 
  kable(.,format  = "markdown",
        col.names = c("Response", "Signal Trial", "Noise Trial"))
```

which corresponds to:

```{r echo=FALSE}
data.frame("R"  = c("Old","New"),
           "ST" = c("Hit","Miss"),
           "NT" = c("False alarm", "correct rejection")) %>% 
  kable(.,format  = "markdown",
        col.names = c("Response", "Signal Trial", "Noise Trial"))
```

This allows us to take the observed number of hits and false alarms in our experiment, and translate them into psychological meaningful parameters $d$ and $c$

## The Experiment         

The experiment was conducted as part of a pre-test to select stimuli for a subsequent multiple-cue judgment experiment. The experiment consisted of five blocks, with two phases each. In the learning phase, participants saw 12 different pictures of either plants, houses, aliens, bugs, or plates, (a different stimulus set in each block) with features differing on five cues. The same 12 pictures were presented 4 times for 5 seconds to each participant. After the learning phase, participants were presented again with all 12 old pictures as well as 20 new pictures in the testing phase. In each trial of the testing phase, participants had to indicate if the picture was an old picture or a new one.  These two phases were repeated for each of the five stimulus sets. 

## The Data set           

The data set contains the following variables:

- `ID`: participants ID
- `block`: block number 
- `trial`: trial number
- `stimulus`: type of stimulus (house, alien, plant, plate, or bug)
- `response`: response given in this trial (old or new)
- `correctResponse`: correct response in this trial
- `correct`: is `true` if response is equal to `correctResponse`, if not is it `false`

```{r readInData, echo=FALSE}

dataSDT <- read.csv2("_R/Data/Stimulus_Test_tidy.csv") %>% 
            select(ID,block,trial,stimulus,response,correctResponse,correct)

dataSDT[1:3,] %>%  kable(.,format  = "markdown")
```

Based on these variables, we can calculate the number of hits, false alarms, false negatives, and misses, as well as their corresponding rates for each person, in each block. In addition, I calculated $d'$  as $d' = z(HR) - z(FR)$ (Snodgrass &  Corwin, 1988; Stanislaw & Todorov,1999), where HR again is the hit rate and FR is the false-alarm rate.

```{r calculateHitandFA} 
hits <- dataSDT %>%  
          mutate(IDn            = as.numeric(as.factor(ID)), 
                 hit            = ifelse(response == "old" & correctResponse == "old",1,0),
                 false_positive = ifelse(response == "old" & correctResponse == "new",1,0)) %>% 
          group_by(ID,IDn,stimulus,block) %>% 
          summarize(n_old    = sum(correctResponse == "old"),
                    n_new    = sum(correctResponse == "new"),
                    h        = sum(hit),
                    fa       = sum(false_positive),
                    hit_rate = sum(hit)/n_old, 
                    fa_rate  = sum(false_positive)/n_new,
                    .groups  = "keep") %>% 
          mutate(hit_rate = case_when(
                               hit_rate == 1 ~ .9999999,
                               hit_rate == 0 ~ .0000001,
                               TRUE ~ hit_rate
                            ),
                 fa_rate = case_when(
                               fa_rate == 1 ~ .9999999,
                               fa_rate == 0 ~ .0000001,
                               TRUE ~ fa_rate
                            ),
                 z_h_rate  = qnorm(hit_rate),
                 z_fa_rate = qnorm(fa_rate),
                 dprime    = z_h_rate-z_fa_rate)  


```

Which then gives us the following data structure:

```{r, echo=FALSE}
hits[1:5,-1] %>% kable(.,format  = "markdown",digits=2) 
```

If we have a quick look at the hit-rate of each person in each of the five stimulus sets, we see that the performance (this is the hit-rate) is rather low. 

```{r plot hitrates}
ggplot(hits,aes(x = hit_rate)) +
  geom_bar(color = "black", fill = "lightgrey") + 
  facet_wrap(.~stimulus) +
  xlim(0,1) +
  labs(x = "Hit-Rate")
```

For the start, let us focus on one of the stimulus sets in our modelling attempts:

```{r selectData}
temp <- hits %>% filter(stimulus == "plants")
```


## SDT - Non Hierarchical - One Stimulus Set 

I will first build the non-hierarchical version of the SDT model as described in Chapter 11 on pages 158-159 in the fantastic book of Lee and Wagenmakers (2014). I will first present the complete Stan code of the model and then will go through it, step by step.

### The Stan-Model 

This is how our SDT model looks in Stan:

```{Stan, output.var="StanModel_oneStim_nonHier"}
data {
  int<lower=0> p;     // number of persons
  int<lower=0> s[p];  // number of signal trials == old trials of person p
  int<lower=0> n[p];  // number of noise  trials == new trials of person p
  int<lower=0> h[p];  // number of hits of person p
  int<lower=0> fa[p]; // number of false alarms of person p
}


parameters {
  vector[p] d;
  vector[p] c;
}


transformed parameters {
  vector<lower=0,upper=1> [p] hit_rate;
  vector<lower=0,upper=1> [p] fa_rate;
 
  for (i in 1:p){
    hit_rate[i] = Phi_approx(d[i]/2-c[i]);
    fa_rate[i]  = Phi_approx(-d[i]/2-c[i]);
  }
}


model {
// Priors
  d ~ normal(0, 1);
  c ~ normal(0, 1);

// Likelihood
  h  ~ binomial(s, hit_rate);
  fa ~ binomial(n, fa_rate);
} 


generated quantities {

  vector<lower=0> [p] h_pred;
  vector<lower=0> [p] fa_pred;
 
  for (i in 1:p){
    h_pred[i]  = binomial_rng(s[i], hit_rate[i]);
    fa_pred[i] = binomial_rng(n[i], fa_rate[i]);
  }
}
```



### The Data       

In the `data` block, I define the data I am using in the model, which is:

- `p` the number of participants
- `s` a vector containing the number of signal trials (i.e., trials with a picture already shown in the learning phase)
- `n` a vector containing the number of noise trials (i.e., the number of new pictures)
- `h` a vector containing the number of hits of each person
- `fa` a vector containing the number if false alarms of each person


```{r StanData}
Stan_data <- list(
  s  = temp$n_old,
  n  = temp$n_new,
  h  = temp$h,
  fa = temp$fa,
  p  = length(unique(temp$ID))
)

Stan_data
```

### The Parameters 

In the `parameters` block, I define the parameters there are in our model and which we are interested in, which is one $d$ and one $c$ parameter for each participant $p$. These  parameters are then transformed in to the individual *hit* and  *false-alarm* rates in the `transformed parameters` block. 


### The Model      

In the `model` block, I then specify the model structure, this is, how the parameters relate to each other, the prior distributions of the parameters, as well as the binomial likelihood function connecting our parameters and our data. This can be written as: 


$$
\begin{aligned}
HR_p &= \phi(0.5 \times d_p - c_p) \\[.5em]
FR_p &= \phi(- 0.5 \times d_p - c_p) 
\\[1.5em]
h_p & \sim binomial(HR_p,s_p)\\[.5em]
fa_p &\sim binomial(FR_p,n_p)
\end{aligned}
$$



In the `generated quantities` block I also include variables for later posterior predictive analysis, capturing the predictions of `h` and `fa` based on the current parameter values of each step of the MCMC-chains. 

### The Priors     

The priors for the individaul $d_p$ and $c_p$ parameters are normal distributions with $\mu = 0$ and $\sigma = 1$

$$
\begin{aligned}
d_p &\sim Normal(0,1)\\[.5em]
c_p &\sim Normal(0,1)
\end{aligned}
$$


which corresponds to a uniform distribution after transforming them into hit and false-alarm rates.


```{r echo=FALSE}
x <- rnorm(1e5)
y <- pnorm(x)

p1 = ggplot(data.frame(x),aes(x = x)) +
  geom_histogram(bins=50,color = "black", alpha = 0.5) +
  labs(x = "parameter",
       title = TeX("Prior on $d_$ and $c_p$")) 

p2 = ggplot(data.frame(y),aes(x = y)) +
  geom_histogram(bins=50,color = "black", alpha = 0.5) +
  labs(x = "parameter",
       title = "Prior on hit/false-alarm rate") 

p1 + p2 #patchwork package
```



## Run the Model 

Now that I have defined the model, the data and the priors, we are ready to go.  I use the newish `CmdStanR` interface to Stan to fit the model. I will draw 10.000 posterior samples, with a rather small warm-up of 2000 iterations and thinning = 4. 

```{r fitmod}
options(mc.cores = 2)

mod  <- cmdstan_model("Stan Models/StanModel_oneStim_nonHier.Stan")

fit1 <- mod$sample(
          data           = Stan_data, # named list of data
          chains         = 2,         # number of Markov chains
          iter_warmup    = 2000,      # number of warmup iterations per chain
          iter_sampling  = 10000,     # total number of iterations per chain
          refresh        = 0,         # no progress shown
          thin           = 4,
          adapt_delta    = 0.9)




```

Luckily, this model took only around 6s to run and not several hours our days, as more complex Bayesian models tend to do. Lets save the model and the posterior draws for later use. 

```{r savemod}

fit1$save_object(file = "Data/fit_StanModel_nonHier.RDS")

```


### Inspect MCMC traces, Rhat, ESS

Before we can dive right in in our posterior distributions and parameter estimates, we should check the convergence of the MCMC chains. First, we can look at some MCMC-Traces for some of the parameters and persons (in my own research, I do this with every single parameter, even when it takes some time). 

```{r, fig.height=4, fig.width=10,dpi=300}

as_draws_df(fit1$draws()) %>% 
 mcmc_trace(posterior_SDT_oS_nH,
            pars = vars("d[1]":"d[3]",
                        "c[1]":"c[3]",
                        "hit_rate[1]":"hit_rate[3]",
                        "fa_rate[1]" :"fa_rate[3]"),
            facet_args = list(nrow = 4, labeller = label_parsed))

```

So far so good, this looks exactly as you would like it, some nice hairy caterpillars. Next, we should have a look at the distributions of $\hat{Rs}$  and the *effective sample size (ESS)*:

```{r, fig.height=4, fig.width=10,dpi=300}

summary_oS_nH  <- fit1$summary() %>% as.data.frame()


p1 <- ggplot(summary_oS_nH,aes(x = ess_bulk))+
        geom_histogram(bins = 40, color = "black", fill = "skyblue2")+
        xlim(0,6000)+
        labs(x = "Effective Sample Size",
             y = "Count")

p2 <- ggplot(summary_oS_nH,aes(x = rhat))+
        geom_histogram(bins = 40, color = "black", fill = "tomato2")+
        xlim(0.99,2) + 
        labs(x = "Rhat",
             y = "Count")

p1+p2 #patchwork package


```

This looks also fine. The effective sample sizes are always > 400, which is often recommended, and are near the total sample size of  `r (10000-2000)/4 * 2`. Also $\hat{R}$ is always very close to 1.

### Posterior Summaries & Densities

Now we come to the interesting part and what we are interested in: the posterior distributions of our parameters. Because plots are fun, lets start with making some plots of the posterior distributions of the $d$ and $c$ parameters of our participants. Here I use the `gather_draw()` function from the `tidybayes` package to prepare the data to make the plot. This function makes it very easy to extract a tidy data.frame when we have parameter names like `d[12]`, where `d` is the name of our parameter and `[12]` indicates the ID of the participant. 


```{r}
df_post <- fit1 %>% gather_draws(d[ID],c[ID]) 

head(df_post)
```

Equipped with this data.frame of posterior draws (`.value`) per person (`ID`) for both parameters (`.variable`) we can plot the posterior distributions:

```{r}
df_post %>% 
  ggplot(aes(y = factor(ID), x = .value)) +
    stat_halfeye() +
    facet_wrap(.~.variable) +
    labs(x = "Value",
         y = "ID")
```

#### Parameter: $d_p$ 

Since $d$ is the parameter we are most interested in, lets start with this one. Below you see a forest plot showing the median, 50%, and 95% credible intervals of the posterior distributions, as well as uni-variate marginal posterior distributions. We can see that the individual $d_p$ values are rather small and very close to 0 for most people. This indicates that participants had a hard time differentiating old from new stimuli. I had hoped for larger values, as it is necessary that people are able to discriminate between stimuli and their features rather well for the multiple-cue judgment experiment I wanted to conduct with these stimuli.


#### Parameter $c_p$

We can also look at the $c$ values. Since we have more noise trials (new stimuli) than signal trials (old stimuli) in our testing phase (20 vs. 12), and participant were told this information, I expected to find slightly positive values of $c$. However, as apparent from the plots, participants had more negative values of $c$, indicating a bias for the "old"-response. Looking at the plot, it is also clear that there are two participants who had smaller $c$ values then the other participants, which are rather similar to each other. 

### The posterior predictive values

Finally,let's have a look at the posterior predictive values of `h` and `fa`. For this, I first combine and then plot the actual observed values from our original data.frame with the MCMC estimates. So lets first create a tidy data.frame only containing the empirical variables we need.

```{r}
pp_emp <- hits  %>%  filter(stimulus == "plants") %>% ungroup() %>% select(.,ID=IDn,h,fa)

head(pp_emp)
```

Next, we extract the posterior draws for our predicted values. 

```{r}
pp_pred <- fit1 %>% spread_draws(fa_pred[ID],h_pred[ID])

head(pp_pred)
```

Now I can simply join both data.frames together:


```{r}
pp_df <- pp_pred %>% left_join(.,pp_emp,by = c("ID")) 

head(pp_df)
```



This data.frame now contains the empirically observed number of hits and false-alarms, as well as the corresponding posterior predicted draws for each participant. We can now plot everything together, to get an idea of how good our (in-sample) posterior predictions capture the (empirical) reality. To safe some space, I will only do the plots for four participants. 


```{r fig.height=4, fig.width=10,dpi=300}

pp1 <- ggplot(filter(pp_df,ID %in% 1:4),aes(x = h_pred)) +
        geom_histogram(bins=10,color = "black",fill = "skyblue2",alpha=0.5) +
        geom_vline(aes(xintercept =  h),color = "black",lwd = 1.5, lty = "dashed")+
        facet_wrap(.~ID,scales="free") 

pp2 <- ggplot(filter(pp_df,ID %in% 1:4),aes(x = fa_pred)) +
        geom_histogram(bins=10,color = "black",fill = "tomato2",alpha=0.5) +
        geom_vline(aes(xintercept =  fa),color = "black",lwd = 1.5, lty = "dashed")+
        facet_wrap(.~ID,scales="free") 

pp1 + pp2 #patchwork package
```

This looks also fine so far. The posterior distribution is symmetrically distributed around the empirical values.  In addition, let me also calculate how often the 95% credible interval of the predicted values contains the true values:

```{r}
fit1 %>%
  spread_draws(fa_pred[ID],h_pred[ID]) %>%
  mean_hdci() %>% 
  left_join(.,pp_emp,by = c("ID"))  %>% 
  summarize(pp_h  = mean(h  > h_pred.lower  & h  < h_pred.upper),
            pp_fa = mean(fa > fa_pred.lower & fa < fa_pred.upper)) %>% 
  as.data.frame() # better output in .md
```

Finally, we can also make a scatterplot plotting the observed against the predicted (i.e., median of the posterior predictive distributions) hit and false-alarm rates for each participant.

```{r}
ph <- fit1 %>%
        spread_draws(h_pred[ID]) %>%
        mean_hdci() %>% 
        left_join(.,pp_emp,by = c("ID")) %>% 
        ggplot(aes(x = h,h_pred)) +
          geom_smooth(method="lm") +
          geom_point(size=2,shape=21,color="black",fill="grey") +
          labs(x = "Observed",
               y = "Predicted",
               title = "Hit-Rate")


pfa <- fit1 %>%
        spread_draws(fa_pred[ID]) %>%
        mean_hdci() %>% 
        left_join(.,pp_emp,by = c("ID")) %>% 
        ggplot(aes(x = fa,fa_pred)) +
          geom_smooth(method="lm") +
          geom_point(size=2,shape=21,color="black",fill="grey") +
          labs(x = "Observed",
               y = "Predicted",
               title = "False-Alarm Rate")

ph+pfa
```


As evident from the plots and the 95% credible interval checks of the posterior predictives, our model is able to recover our data rather well. 


In the following blog posts I now want to check if the hierarchical SDT model or other model families (2HTM) are better able to recover our data.



---

### References 

- Lee, M. D., & Wagenmakers, E. J. (2014). *Bayesian cognitive modeling: A practical course.* Cambridge university press.

- Snodgrass, J. G., & Corwin, J. (1988). Perceptual Identification Thresholds for 150 Fragmented Pictures from the Snodgrass and Vanderwart Picture Set. *Perceptual and Motor Skills*, 67(1), 3–36. https://doi.org/10.2466/pms.1988.67.1.3

- Stanislaw, H., & Todorov, N. (1999). Calculation of signal detection theory measures. *Behavior Research Methods, Instruments, & Computers*, 31(1), 137–149. https://doi.org/10.3758/BF03207704


```{r, child = "_footer.Rmd"}
```
